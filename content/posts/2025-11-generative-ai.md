
---
title: What's
date: 2025-11-17
draft: true
description: Some takes on AI and AI discourse in 2025 Q4.
---

I had the idea to write out my thoughts in order to have a concrete document to revisit in future years. It can be easy to get revisionist about what you believed and when, so here is a list of things I currently believe are true after 2ish years of pretty extensive experience with frontier models, especially Anthropic's Claude.

## Takes

* AI is very good at all sorts of things. It is especially good at writing code. I've also used it for dealing with legal documents, talking through appliance repair, learning about history, and giving tax advice.

* We are not in a bubble comparable to previous bubbles. The market is overheated, and some irrespinsible particpants will suffer when their bad bets fail, but I do not believe a sector-wide collapse is likely.

* Environmental concerns about AI and Data Centers [often rely on misinformation](https://andymasley.substack.com/p/a-short-summary-of-my-argument-that). Environmental harms are important to mitigate, and more intellectual integrity on this topic would help us effectively mitigate them.

* Because the US government is failing to deliver adequate energy infrastructure, AI companies may be accelerating the green energy transition as they partner directly with utilities and build their own.

* Using AI to produce art, especially that which supplants human artists, is immoral and worthy of social sanctions. Artists have a unique and important role in a healthy society. They sustain themselves through their work products, but those products are only side effects of the value that artists provide. 

* Playing around and making stuf for personal use or amusement is pretty benign, though.

* AI boosters are all selling something and are inherently untrustworthy. Because most of the experts on the technology have conflicts of interest.

* AI skeptics often confuse material debates about the merits of AI with broad philosophical or moral debates about capitalism. If someone's priors are that "Capitalism has produced nothing of value" and capitalism produces AI, then "AI must have no value". Such skeptics might argue that "AI hallucinates everything" as a proxy for "AI might accelerate wealth inequality and we should boycott it".

* AI may accelerate wealth inequality. The skeptics are correct about this.

* AI will cause (mostly psychological) harm to humans in similar ways and at similar rates to other digital technologies (smartphones and the internet). Society will mostly tolerate these harms and ignore them, just as it ignores the 1 million annual worldwide deaths from cars.

* Large language models are not about to become sentient, have feelings, or experience psychological (or physical) pain. [It's still reasonable to be conerned about this longer term.](https://alleninstitute.org/news/one-of-worlds-most-detailed-virtual-brain-simulations-is-changing-how-we-study-the-brain/)

* AI does not have intentions or desires. It will not want to harm us because it cannot want. It is a mathmatical model organized to engage in role-playing games. We anthropomorphise everything at our own peril.

* We have avoided nuclear holocaust thus far by luck. Nuclear disarmament is the only safe course of action available to us. Nuclear weapons are not compatible with human flourishing long term whether we have advanced AI or not, but having AI greatly increases the risk.

* It is not possible to make "safe" AI. We will probably have AI that is mostly safe most of the time, and then sometimes it will malfunction and cause a an electricity blackout. Humans must retain localized physical control of critical infrastructure, weapons systems, etc. and should build physical emergency cutoffs at datacenters.

## Disclaimers

Qualifiers like "maybe" or "likely" were omitted because I am uncertain about everything above, and otherwise I would have placed them everywhere. My goal was to stake out some positions, not hedge against being wrong.

I am neither a skeptic nor a booster. I think these technologies are exceedingly _interesting_, and not inerently good or bad. Rather, I think particular applications of the tools are good (fixing your dryer) or bad (selling slop art).

Finally, I think we exist in a dynamic period where challengers across domains could leverage AI against lethargic incumbents. I think a lot of civic and social good could result if people with laudable goals would be willing weild AI in service of their desired outcomes rather than include AI on their enemies lists. There's plenty of reason to be angry at Meta, for example, but it doesn't make sense to stubbornly refuse to use Llama, PyTorch, or React in service of a better future.