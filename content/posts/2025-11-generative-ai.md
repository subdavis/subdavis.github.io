
---
title: Thinking about Generative AI
date: 2025-11-23
description: A time capsule journal entry
---

> Long before we get to the point where a machine is a moral agent, we will have machines that are capable of suffering. - Ted Chiang

I want to write out some thoughts to revisit later. It can be easy to get revisionist about what you believed and when, so here is a list of things I thought were true in late 2025 after ~2 years of pretty extensive experience with frontier models, especially Anthropic's Claude.

## Takes

* AI is alarmingly good at all sorts of things. It is good at writing code, dealing with legal documents, talking through household repairs, and many other tasks.

* We are not in a bubble comparable to previous bubbles, but the market is overheated. Many participants will suffer when their bad bets fail. A sector-wide collapse followed by another "AI winter" is not likely, but a recession that affects regular American families feels very possible.

* Environmental concerns about AI and Data Centers are often [partially based on misinformation](https://andymasley.substack.com/p/individual-ai-use-is-not-bad-for). Environmental harms are important to mitigate, and more intellectual integrity on this topic would help us effectively mitigate them.

* As the US government actively sabotages renewable energy infrastructure projects, power demands from AI companies is both [applying positive pressure toward renewable energy transition](https://investors.constellationenergy.com/news-releases/news-release-details/constellation-meta-sign-20-year-deal-clean-reliable-nuclear) and [keeping fossil fuel plants online longer](https://stateimpactcenter.org/insights/data-centers-straining-the-grid-and-your-wallet) than they otherwise would be.

* Using AI to produce art, especially that which supplants human artists, is bad. Artists have a unique and important role in a healthy society. They sustain themselves through their work products. It's clear that this is already happening, and this is bad.

* Playing around and making art for personal use or amusement is mostly benign, though I still have reservations.

* AI boosters are  selling something and are untrustworthy. Most of the experts on the technology have conflicts of interest, and their incentives are not aligned with public good. We need more independent experts outside the AI industry.

* AI skeptics often confuse material debates about the merits of AI with larger philosophical or moral debates about capitalism. If someone's priors are that "capitalism has rendered nothing of value" and capitalism produces AI, then AI must have no value. Such skeptics might say that "AI hallucinates everything" as evidence to support a boycott. This is misleading. They favor a boycott on philosophical grounds, and the "hallucination" argument is a red herring.

* AI will, in the near term, accelerate wealth inequality. Skeptics are correct about this. Longer term effects depend on what we decide to do as a society.

* AI will cause (mostly psychological) harm to humans at higher rates than other digital technologies. It will make harms from social media or smartphone use look quaint just as those overshadowed concerns about television. Society will mostly tolerate these harms and ignore them, just as it ignores the 1 million annual worldwide deaths from automobiles (40,000 in the US).

* Large language models are not on the cusp of becoming self-aware or developing the capacity to suffer, but [I am still worried that we might cause machines to experience suffering in the future.](https://alleninstitute.org/news/one-of-worlds-most-detailed-virtual-brain-simulations-is-changing-how-we-study-the-brain/)

* We have avoided nuclear holocaust thus far by luck. Nuclear disarmament should be an urgent priority. All sides of the AI discourse should agree on this.

## Disclaimers

Qualifiers like "maybe" or "likely" were omitted because I am uncertain about everything, and otherwise I would have placed them everywhere. My goal was to stake out some positions, not hedge against being wrong.

I am neither a skeptic nor a booster. I think these technologies are exceedingly _interesting_. The producers of LLMs are mostly untrustworthy, but using the technology does not make you complicit in the way that shopping on Amazon does, for example. I [dislike most of what Big Tech has done lately](/posts/2025-11-introduction/).

We exist in a dynamic period where challengers across domains could leverage AI against more lethargic incumbents. Civic and social good might result if good people would be willing to selectively wield AI in service of their desired outcomes. There's plenty of reason to be angry at Meta, for example, but it doesn't make sense to stubbornly refuse to use [Llama](https://www.llama.com/), [PyTorch](https://pytorch.org/), [sam3](https://ai.meta.com/sam3/), or React in pursuit of a better future.
